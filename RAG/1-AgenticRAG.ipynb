{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c314bb1",
   "metadata": {},
   "source": [
    "# Agentic RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac37003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a77c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad4cda",
   "metadata": {},
   "source": [
    "### Create Vector DB for LangGraph Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1865b816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangGraph overview - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langgraph\\n\\nThen, create a simple hello world example:\\nCopyAsk AIfrom langgraph.graph import StateGraph, MessagesState, START, END\\n\\ndef mock_llm(state: MessagesState):\\n    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\\n\\ngraph = StateGraph(MessagesState)\\ngraph.add_node(mock_llm)\\ngraph.add_edge(START, \"mock_llm\")\\ngraph.add_edge(\"mock_llm\", END)\\ngraph = graph.compile()\\n\\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\\n\\n\\u200bCore benefits\\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:\\n\\nDurable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\\nHuman-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\\nComprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\\nDebugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\\nProduction-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\\n\\n\\u200bLangGraph ecosystem\\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:\\n\\nLangSmith — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\nLangSmith — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.\\nLangChain - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\n\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat\\'s new in v1NextDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/workflows-agents', 'title': 'Workflows and agents - Docs by LangChain', 'language': 'en'}, page_content='Workflows and agents - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\\n\\nWorkflows have predetermined code paths and are designed to operate in a certain order.\\nAgents are dynamic and define their own processes and tool usage.\\n\\n\\nLangGraph offers several benefits when building agents and workflows, including persistence, streaming, and support for debugging as well as deployment.\\n\\u200bSetup\\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:\\n\\nInstall dependencies:\\n\\nCopyAsk AIpip install langchain_core langchain-anthropic langgraph\\n\\n\\nInitialize the LLM:\\n\\nCopyAsk AIimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\\n\\n\\u200bLLMs and augmentations\\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.\\n\\nCopyAsk AI# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\n\\u200bPrompt chaining\\nPrompt chaining is when each LLM call processes the output of the previous call. It’s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\\n\\nTranslating documents into different languages\\nVerifying generated content for consistency\\n\\n\\nGraph APIFunctional APICopyAsk AIfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\n\\u200bParallelization\\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\\n\\nSplit up subtasks and run them in parallel, which increases speed\\nRun tasks multiple times to check for different outputs, which increases confidence\\n\\nSome examples include:\\n\\nRunning one subtask that processes a document for keywords, and a second subtask to check for formatting errors\\nRunning a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\\n\\n\\nGraph APIFunctional APICopyAsk AI# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\n\\u200bRouting\\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\\n\\nGraph APIFunctional APICopyAsk AIfrom typing_extensions import Literal\\nfrom langchain.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\n\\u200bOrchestrator-worker\\nIn an orchestrator-worker configuration, the orchestrator:\\n\\nBreaks down tasks into subtasks\\nDelegates subtasks to workers\\nSynthesizes worker outputs into a final result\\n\\n\\nOrchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with parallelization. This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.\\nGraph APIFunctional APICopyAsk AIfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\u200bCreating workers in LangGraph\\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. The Send API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send API to send a section to each worker.\\nCopyAsk AIfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\n\\u200bEvaluator-optimizer\\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\\nEvaluator-optimizer workflows are commonly used when there’s particular success criteria for a task, but iteration is required to meet that criteria. For example, there’s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\\n\\nGraph APIFunctional APICopyAsk AI# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\n\\u200bAgents\\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\\n\\nTo get started with agents, see the quickstart or read more about how they work in LangChain.\\nUsing toolsCopyAsk AIfrom langchain.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide `a` and `b`.\\n\\n    Args:\\n        a: First int\\n        b: Second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional APICopyAsk AIfrom langgraph.graph import MessagesState\\nfrom langchain.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"tool_node\"\\n\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"tool_node\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    [\"tool_node\", END]\\n)\\nagent_builder.add_edge(\"tool_node\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoThinking in LangGraphPreviousPersistenceNextDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph#start-with-the-process-you-want-to-automate', 'title': 'Thinking in LangGraph - Docs by LangChain', 'description': 'Learn how to think about building agents with LangGraph by breaking down a customer support email agent into discrete steps', 'language': 'en'}, page_content='Thinking in LangGraph - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedThinking in LangGraphLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this pageStart with the process you want to automateStep 1: Map out your workflow as discrete stepsStep 2: Identify what each step needs to doLLM StepsData StepsAction StepsUser Input StepsStep 3: Design your stateWhat belongs in state?Keep state raw, format prompts on-demandStep 4: Build your nodesHandle errors appropriatelyImplementing our email agent nodesStep 5: Wire it togetherTry out your agentSummary and next stepsKey InsightsAdvanced considerationsWhere to go from hereGet startedThinking in LangGraphCopy pageLearn how to think about building agents with LangGraph by breaking down a customer support email agent into discrete stepsCopy pageLangGraph can change how you think about the agents you build. When you build an agent with LangGraph, you will first break it apart into discrete steps called nodes. Then, you will describe the different decisions and transitions for each of your nodes. Finally, you will connect your nodes together through a shared state that each node can read from and write to. In this tutorial, we’ll guide you through the thought process of building a customer support email agent with LangGraph.\\n\\u200bStart with the process you want to automate\\nImagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements:\\nThe agent should:\\n\\nRead incoming customer emails\\nClassify them by urgency and topic\\nSearch relevant documentation to answer questions\\nDraft appropriate responses\\nEscalate complex issues to human agents\\nSchedule follow-ups when needed\\n\\nExample scenarios to handle:\\n\\nSimple product question: “How do I reset my password?”\\nBug report: “The export feature crashes when I select PDF format”\\nUrgent billing issue: “I was charged twice for my subscription!”\\nFeature request: “Can you add dark mode to the mobile app?”\\nComplex technical issue: “Our API integration fails intermittently with 504 errors”\\n\\nTo implement an agent in LangGraph, you will usually follow the same five steps.\\n\\u200bStep 1: Map out your workflow as discrete steps\\nStart by identifying the distinct steps in your process. Each step will become a node (a function that does one specific thing). Then sketch how these steps connect to each other.\\n\\nThe arrows show possible paths, but the actual decision of which path to take happens inside each node.\\nNow that you’ve identified the components in your workflow, let’s understand what each node needs to do:\\n\\nRead Email: Extract and parse the email content\\nClassify Intent: Use an LLM to categorize urgency and topic, then route to appropriate action\\nDoc Search: Query your knowledge base for relevant information\\nBug Track: Create or update issue in tracking system\\nDraft Reply: Generate an appropriate response\\nHuman Review: Escalate to human agent for approval or handling\\nSend Reply: Dispatch the email response\\n\\nNotice that some nodes make decisions about where to go next (Classify Intent, Draft Reply, Human Review), while others always proceed to the same next step (Read Email always goes to Classify Intent, Doc Search always goes to Draft Reply).\\n\\u200bStep 2: Identify what each step needs to do\\nFor each node in your graph, determine what type of operation it represents and what context it needs to work properly.\\nLLM StepsUse when you need to understand, analyze, generate text, or make reasoning decisionsData StepsUse when you need to retrieve information from external sourcesAction StepsUse when you need to perform external actionsUser Input StepsUse when you need human intervention\\n\\u200bLLM Steps\\nWhen a step needs to understand, analyze, generate text, or make reasoning decisions:\\nClassify Intent Node\\nStatic context (prompt): Classification categories, urgency definitions, response format\\nDynamic context (from state): Email content, sender information\\nDesired outcome: Structured classification that determines routing\\nDraft Reply Node\\nStatic context (prompt): Tone guidelines, company policies, response templates\\nDynamic context (from state): Classification results, search results, customer history\\nDesired outcome: Professional email response ready for review\\n\\n\\u200bData Steps\\nWhen a step needs to retrieve information from external sources:\\nDocument Search Node\\nParameters: Query built from intent and topic\\nRetry strategy: Yes, with exponential backoff for transient failures\\nCaching: Could cache common queries to reduce API calls\\nCustomer History Lookup\\nParameters: Customer email or ID from state\\nRetry strategy: Yes, but with fallback to basic info if unavailable\\nCaching: Yes, with time-to-live to balance freshness and performance\\n\\n\\u200bAction Steps\\nWhen a step needs to perform an external action:\\nSend Reply Node\\nWhen to execute: After approval (human or automated)\\nRetry strategy: Yes, with exponential backoff for network issues\\nShould not cache: Each send is a unique action\\nBug Track Node\\nWhen to execute: Always when intent is “bug”\\nRetry strategy: Yes, critical to not lose bug reports\\nReturns: Ticket ID to include in response\\n\\n\\u200bUser Input Steps\\nWhen a step needs human intervention:\\nHuman Review Node\\nContext for decision: Original email, draft response, urgency, classification\\nExpected input format: Approval boolean plus optional edited response\\nWhen triggered: High urgency, complex issues, or quality concerns\\n\\n\\u200bStep 3: Design your state\\nState is the shared memory accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.\\n\\u200bWhat belongs in state?\\nAsk yourself these questions about each piece of data:\\nInclude in StateDoes it need to persist across steps? If yes, it goes in state.Don\\'t StoreCan you derive it from other data? If yes, compute it when needed instead of storing it in state.\\nFor our email agent, we need to track:\\n\\nThe original email and sender info (can’t reconstruct these)\\nClassification results (needed by multiple downstream nodes)\\nSearch results and customer data (expensive to re-fetch)\\nThe draft response (needs to persist through review)\\nExecution metadata (for debugging and recovery)\\n\\n\\u200bKeep state raw, format prompts on-demand\\nA key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.\\nThis separation means:\\n\\nDifferent nodes can format the same data differently for their needs\\nYou can change prompt templates without modifying your state schema\\nDebugging is clearer - you see exactly what data each node received\\nYour agent can evolve without breaking existing state\\n\\nLet’s define our state:\\nCopyAsk AIfrom typing import TypedDict, Literal\\n\\n# Define the structure for email classification\\nclass EmailClassification(TypedDict):\\n    intent: Literal[\"question\", \"bug\", \"billing\", \"feature\", \"complex\"]\\n    urgency: Literal[\"low\", \"medium\", \"high\", \"critical\"]\\n    topic: str\\n    summary: str\\n\\nclass EmailAgentState(TypedDict):\\n    # Raw email data\\n    email_content: str\\n    sender_email: str\\n    email_id: str\\n\\n    # Classification result\\n    classification: EmailClassification | None\\n\\n    # Raw search/API results\\n    search_results: list[str] | None  # List of raw document chunks\\n    customer_history: dict | None  # Raw customer data from CRM\\n\\n    # Generated content\\n    draft_response: str | None\\n    messages: list[str] | None\\n\\nNotice that the state contains only raw data - no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM.\\n\\u200bStep 4: Build your nodes\\nNow we implement each step as a function. A node in LangGraph is just a Python function that takes the current state and returns updates to it.\\n\\u200bHandle errors appropriately\\nDifferent errors need different handling strategies:\\n\\nError TypeWho Fixes ItStrategyWhen to Use\\nTransient errors (network issues, rate limits)System (automatic)Retry policyTemporary failures that usually resolve on retryLLM-recoverable errors (tool failures, parsing issues)LLMStore error in state and loop backLLM can see the error and adjust its approachUser-fixable errors (missing information, unclear instructions)HumanPause with interrupt()Need user input to proceedUnexpected errorsDeveloperLet them bubble upUnknown issues that need debugging\\n\\n Transient errors LLM-recoverable User-fixable UnexpectedAdd a retry policy to automatically retry network issues and rate limits:CopyAsk AIfrom langgraph.types import RetryPolicy\\n\\nworkflow.add_node(\\n    \"search_documentation\",\\n    search_documentation,\\n    retry_policy=RetryPolicy(max_attempts=3, initial_interval=1.0)\\n)\\n\\n\\u200bImplementing our email agent nodes\\nWe’ll implement each node as a simple function. Remember: nodes take state, do work, and return updates.\\nRead and classify nodesCopyAsk AIfrom typing import Literal\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import interrupt, Command, RetryPolicy\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain_core.messages import HumanMessage\\n\\nllm = ChatOpenAI(model=\"gpt-4\")\\n\\ndef read_email(state: EmailAgentState) -> dict:\\n    \"\"\"Extract and parse email content\"\"\"\\n    # In production, this would connect to your email service\\n    return {\\n        \"messages\": [HumanMessage(content=f\"Processing email: {state[\\'email_content\\']}\")]\\n    }\\n\\ndef classify_intent(state: EmailAgentState) -> Command[Literal[\"search_documentation\", \"human_review\", \"draft_response\", \"bug_tracking\"]]:\\n    \"\"\"Use LLM to classify email intent and urgency, then route accordingly\"\"\"\\n\\n    # Create structured LLM that returns EmailClassification dict\\n    structured_llm = llm.with_structured_output(EmailClassification)\\n\\n    # Format the prompt on-demand, not stored in state\\n    classification_prompt = f\"\"\"\\n    Analyze this customer email and classify it:\\n\\n    Email: {state[\\'email_content\\']}\\n    From: {state[\\'sender_email\\']}\\n\\n    Provide classification including intent, urgency, topic, and summary.\\n    \"\"\"\\n\\n    # Get structured response directly as dict\\n    classification = structured_llm.invoke(classification_prompt)\\n\\n    # Determine next node based on classification\\n    if classification[\\'intent\\'] == \\'billing\\' or classification[\\'urgency\\'] == \\'critical\\':\\n        goto = \"human_review\"\\n    elif classification[\\'intent\\'] in [\\'question\\', \\'feature\\']:\\n        goto = \"search_documentation\"\\n    elif classification[\\'intent\\'] == \\'bug\\':\\n        goto = \"bug_tracking\"\\n    else:\\n        goto = \"draft_response\"\\n\\n    # Store classification as a single dict in state\\n    return Command(\\n        update={\"classification\": classification},\\n        goto=goto\\n    )\\nSearch and tracking nodesCopyAsk AIdef search_documentation(state: EmailAgentState) -> Command[Literal[\"draft_response\"]]:\\n    \"\"\"Search knowledge base for relevant information\"\"\"\\n\\n    # Build search query from classification\\n    classification = state.get(\\'classification\\', {})\\n    query = f\"{classification.get(\\'intent\\', \\'\\')} {classification.get(\\'topic\\', \\'\\')}\"\\n\\n    try:\\n        # Implement your search logic here\\n        # Store raw search results, not formatted text\\n        search_results = [\\n            \"Reset password via Settings > Security > Change Password\",\\n            \"Password must be at least 12 characters\",\\n            \"Include uppercase, lowercase, numbers, and symbols\"\\n        ]\\n    except SearchAPIError as e:\\n        # For recoverable search errors, store error and continue\\n        search_results = [f\"Search temporarily unavailable: {str(e)}\"]\\n\\n    return Command(\\n        update={\"search_results\": search_results},  # Store raw results or error\\n        goto=\"draft_response\"\\n    )\\n\\ndef bug_tracking(state: EmailAgentState) -> Command[Literal[\"draft_response\"]]:\\n    \"\"\"Create or update bug tracking ticket\"\"\"\\n\\n    # Create ticket in your bug tracking system\\n    ticket_id = \"BUG-12345\"  # Would be created via API\\n\\n    return Command(\\n        update={\\n            \"search_results\": [f\"Bug ticket {ticket_id} created\"],\\n            \"current_step\": \"bug_tracked\"\\n        },\\n        goto=\"draft_response\"\\n    )\\nResponse nodesCopyAsk AIdef draft_response(state: EmailAgentState) -> Command[Literal[\"human_review\", \"send_reply\"]]:\\n    \"\"\"Generate response using context and route based on quality\"\"\"\\n\\n    classification = state.get(\\'classification\\', {})\\n\\n    # Format context from raw state data on-demand\\n    context_sections = []\\n\\n    if state.get(\\'search_results\\'):\\n        # Format search results for the prompt\\n        formatted_docs = \"\\\\n\".join([f\"- {doc}\" for doc in state[\\'search_results\\']])\\n        context_sections.append(f\"Relevant documentation:\\\\n{formatted_docs}\")\\n\\n    if state.get(\\'customer_history\\'):\\n        # Format customer data for the prompt\\n        context_sections.append(f\"Customer tier: {state[\\'customer_history\\'].get(\\'tier\\', \\'standard\\')}\")\\n\\n    # Build the prompt with formatted context\\n    draft_prompt = f\"\"\"\\n    Draft a response to this customer email:\\n    {state[\\'email_content\\']}\\n\\n    Email intent: {classification.get(\\'intent\\', \\'unknown\\')}\\n    Urgency level: {classification.get(\\'urgency\\', \\'medium\\')}\\n\\n    {chr(10).join(context_sections)}\\n\\n    Guidelines:\\n    - Be professional and helpful\\n    - Address their specific concern\\n    - Use the provided documentation when relevant\\n    \"\"\"\\n\\n    response = llm.invoke(draft_prompt)\\n\\n    # Determine if human review needed based on urgency and intent\\n    needs_review = (\\n        classification.get(\\'urgency\\') in [\\'high\\', \\'critical\\'] or\\n        classification.get(\\'intent\\') == \\'complex\\'\\n    )\\n\\n    # Route to appropriate next node\\n    goto = \"human_review\" if needs_review else \"send_reply\"\\n\\n    return Command(\\n        update={\"draft_response\": response.content},  # Store only the raw response\\n        goto=goto\\n    )\\n\\ndef human_review(state: EmailAgentState) -> Command[Literal[\"send_reply\", END]]:\\n    \"\"\"Pause for human review using interrupt and route based on decision\"\"\"\\n\\n    classification = state.get(\\'classification\\', {})\\n\\n    # interrupt() must come first - any code before it will re-run on resume\\n    human_decision = interrupt({\\n        \"email_id\": state.get(\\'email_id\\',\\'\\'),\\n        \"original_email\": state.get(\\'email_content\\',\\'\\'),\\n        \"draft_response\": state.get(\\'draft_response\\',\\'\\'),\\n        \"urgency\": classification.get(\\'urgency\\'),\\n        \"intent\": classification.get(\\'intent\\'),\\n        \"action\": \"Please review and approve/edit this response\"\\n    })\\n\\n    # Now process the human\\'s decision\\n    if human_decision.get(\"approved\"):\\n        return Command(\\n            update={\"draft_response\": human_decision.get(\"edited_response\", state.get(\\'draft_response\\',\\'\\'))},\\n            goto=\"send_reply\"\\n        )\\n    else:\\n        # Rejection means human will handle directly\\n        return Command(update={}, goto=END)\\n\\ndef send_reply(state: EmailAgentState) -> dict:\\n    \"\"\"Send the email response\"\"\"\\n    # Integrate with email service\\n    print(f\"Sending reply: {state[\\'draft_response\\'][:100]}...\")\\n    return {}\\n\\n\\u200bStep 5: Wire it together\\nNow we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges.\\nTo enable human-in-the-loop with interrupt(), we need to compile with a checkpointer to save state between runs:\\nGraph compilation codeCopyAsk AIfrom langgraph.checkpoint.memory import MemorySaver\\nfrom langgraph.types import RetryPolicy\\n\\n# Create the graph\\nworkflow = StateGraph(EmailAgentState)\\n\\n# Add nodes with appropriate error handling\\nworkflow.add_node(\"read_email\", read_email)\\nworkflow.add_node(\"classify_intent\", classify_intent)\\n\\n# Add retry policy for nodes that might have transient failures\\nworkflow.add_node(\\n    \"search_documentation\",\\n    search_documentation,\\n    retry_policy=RetryPolicy(max_attempts=3)\\n)\\nworkflow.add_node(\"bug_tracking\", bug_tracking)\\nworkflow.add_node(\"draft_response\", draft_response)\\nworkflow.add_node(\"human_review\", human_review)\\nworkflow.add_node(\"send_reply\", send_reply)\\n\\n# Add only the essential edges\\nworkflow.add_edge(START, \"read_email\")\\nworkflow.add_edge(\"read_email\", \"classify_intent\")\\nworkflow.add_edge(\"send_reply\", END)\\n\\n# Compile with checkpointer for persistence, in case run graph with Local_Server --> Please compile without checkpointer\\nmemory = MemorySaver()\\napp = workflow.compile(checkpointer=memory)\\n\\nThe graph structure is minimal because routing happens inside nodes through Command objects. Each node declares where it can go using type hints like Command[Literal[\"node1\", \"node2\"]], making the flow explicit and traceable.\\n\\u200bTry out your agent\\nLet’s run our agent with an urgent billing issue that needs human review:\\nTesting the agentCopyAsk AI# Test with an urgent billing issue\\ninitial_state = {\\n    \"email_content\": \"I was charged twice for my subscription! This is urgent!\",\\n    \"sender_email\": \"customer@example.com\",\\n    \"email_id\": \"email_123\",\\n    \"messages\": []\\n}\\n\\n# Run with a thread_id for persistence\\nconfig = {\"configurable\": {\"thread_id\": \"customer_123\"}}\\nresult = app.invoke(initial_state, config)\\n# The graph will pause at human_review\\nprint(f\"Draft ready for review: {result[\\'draft_response\\'][:100]}...\")\\n\\n# When ready, provide human input to resume\\nfrom langgraph.types import Command\\n\\nhuman_response = Command(\\n    resume={\\n        \"approved\": True,\\n        \"edited_response\": \"We sincerely apologize for the double charge. I\\'ve initiated an immediate refund...\"\\n    }\\n)\\n\\n# Resume execution\\nfinal_result = app.invoke(human_response, config)\\nprint(f\"Email sent successfully!\")\\n\\nThe graph pauses when it hits interrupt(), saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The thread_id ensures all state for this conversation is preserved together.\\n\\u200bSummary and next steps\\n\\u200bKey Insights\\nBuilding this email agent has shown us the LangGraph way of thinking:\\nBreak into discrete stepsEach node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps.State is shared memoryStore raw data, not formatted text. This lets different nodes use the same information in different ways.Nodes are functionsThey take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination.Errors are part of the flowTransient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging.Human input is first-classThe interrupt() function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.Graph structure emerges naturallyYou define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.\\n\\u200bAdvanced considerations\\nNode granularity trade-offsThis section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.You might wonder: why not combine Read Email and Classify Intent into one node?Or why separate Doc Search from Draft Reply?The answer involves trade-offs between resilience and observability.The resilience consideration: LangGraph’s durable execution creates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node.Why we chose this breakdown for the email agent:\\n\\nIsolation of external services: Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others.\\n\\n\\nIntermediate visibility: Having Classify Intent as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring—you can see exactly when and why the agent routes to human review.\\n\\n\\nDifferent failure modes: LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently.\\n\\n\\nReusability and testing: Smaller nodes are easier to test in isolation and reuse in other workflows.\\n\\nA different valid approach: You could combine Read Email and Classify Intent into a single node. You’d lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off.Application-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements—LangGraph doesn’t prescribe this.Performance considerations: More nodes doesn’t mean slower execution. LangGraph writes checkpoints in the background by default (async durability mode), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed—use \"exit\" mode to checkpoint only at completion, or \"sync\" mode to block execution until each checkpoint is written.\\n\\u200bWhere to go from here\\nThis was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:\\nHuman-in-the-loop patternsLearn how to add tool approval before execution, batch approval, and other patternsSubgraphsCreate subgraphs for complex multi-step operationsStreamingAdd streaming to show real-time progress to usersObservabilityAdd observability with LangSmith for debugging and monitoringTool IntegrationIntegrate more tools for web search, database queries, and API callsRetry LogicImplement retry logic with exponential backoff for failed operations\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoRun a local serverPreviousWorkflows and agentsNextDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/workflows-agents\",\n",
    "    \"https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph#start-with-the-process-you-want-to-automate\"\n",
    "]\n",
    "\n",
    "docs = WebBaseLoader(urls).load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e29274c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"LangGraph overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "doc_splits[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "451f4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Createing a vector store and storing it\n",
    "langraph_vector_store = FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    "\n",
    ")\n",
    "\n",
    "langgraph_retriever = langraph_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a782f2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0b910d27-4a9c-4176-90e9-f432bc18a1d1', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content=\"LangGraph overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please\"),\n",
       " Document(id='12d572e1-abed-4eff-b082-952b16a38225', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='Trusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langgraph'),\n",
       " Document(id='e7ffcaed-950e-4b60-9e68-05625259fef2', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph#start-with-the-process-you-want-to-automate', 'title': 'Thinking in LangGraph - Docs by LangChain', 'description': 'Learn how to think about building agents with LangGraph by breaking down a customer support email agent into discrete steps', 'language': 'en'}, page_content=\"Thinking in LangGraph - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedThinking in LangGraphLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this pageStart with the process you want to automateStep 1: Map out your workflow as discrete stepsStep 2: Identify what each step needs to doLLM StepsData StepsAction StepsUser Input StepsStep 3: Design your stateWhat belongs in state?Keep state raw, format prompts on-demandStep 4: Build\"),\n",
       " Document(id='6ce8ac96-2fed-4317-bcdf-ce4a34ca4873', metadata={'source': 'https://docs.langchain.com/oss/python/langgraph/overview', 'title': 'LangGraph overview - Docs by LangChain', 'language': 'en'}, page_content='LangSmith — Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\nLangSmith — Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.\\nLangChain - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.\\n\\n\\u200bAcknowledgements\\nLangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the retriever\n",
    "langgraph_retriever.invoke(\"what is langgraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c76c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "langgraph_retriever_tool = create_retriever_tool(langgraph_retriever,\n",
    "                                                 \"retriever_vector_db_blog\",\n",
    "                                                 \"Search and learn information about langgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff9efee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_db_blog', description='Search and run information about langgraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000244D6A423E0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002452D41E2A0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000244D721C5E0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002452D41E2A0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langgraph_retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72536c",
   "metadata": {},
   "source": [
    "### Create Vector DB for langchain Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cca2cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/overview', 'title': 'LangChain overview - Docs by LangChain', 'language': 'en'}, page_content='LangChain overview - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChain overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityEnglishcloseOn this page Install Create an agent Core benefitsLangChain overviewCopy pageCopy pageLangChain v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please open an issue so we can improve. To view v0.x documentation, go to the archived content.\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use LangGraph, our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\nLangChain agents are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langchain\\n# Requires Python 3.10+\\n\\n\\u200b Create an agent\\nCopyAsk AI# pip install -qU \"langchain[anthropic]\" to call the model\\n\\nfrom langchain.agents import create_agent\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get weather for a given city.\"\"\"\\n    return f\"It\\'s always sunny in {city}!\"\\n\\nagent = create_agent(\\n    model=\"claude-sonnet-4-5-20250929\",\\n    tools=[get_weather],\\n    system_prompt=\"You are a helpful assistant\",\\n)\\n\\n# Run the agent\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\\n)\\n\\n\\u200b Core benefits\\nStandard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.Learn moreEasy to use, highly flexible agentLangChain’s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.Learn moreBuilt on top of LangGraphLangChain’s agents are built on top of LangGraph. This allows us to take advantage of LangGraph’s durable execution, human-in-the-loop support, persistence, and more.Learn moreDebug with LangSmithGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.Learn more\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoWhat\\'s new in v1NextDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/rag', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentSupervisor agentLangGraphConceptual overviewsMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesEnglishcloseOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce we’ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you’re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide we’ll build an app that answers questions about the website’s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyAsk AIimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyAsk AIquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopyAsk AI================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvcondaCopyAsk AIpip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyAsk AIexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyAsk AIimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChain’s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock👉 Read the OpenAI chat model integration docsCopyAsk AIpip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyAsk AIimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx FakeCopyAsk AIpip install -U \"langchain-openai\"\\nCopyAsk AIimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\n\\nSelect a vector store:\\n In-memory AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopyAsk AIpip install -U \"langchain-core\"\\nCopyAsk AIfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you’re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won’t fit in a model’s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case we’ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we’ll remove all others.\\nCopyAsk AIimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyAsk AITotal characters: 43131\\n\\nCopyAsk AIprint(docs[0].page_content[:500])\\n\\nCopyAsk AI      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyAsk AIfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopyAsk AISplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopyAsk AIdocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopyAsk AI[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyAsk AIfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding arguments— for example, a category:CopyAsk AIfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyAsk AIfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLet’s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyAsk AIquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopyAsk AI================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directly— for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph’s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\n\\n✅ Benefits⚠️ Drawbacks\\nSearch only when needed – The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls – When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries – By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control – The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed – The LLM can execute several searches in support of a single user query.\\n\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyAsk AIfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLet’s try this out:\\nCopyAsk AIquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopyAsk AI================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyAsk AIfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that we’ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployments\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/oss/python/langchain/short-term-memory', 'title': 'Short-term memory - Docs by LangChain', 'language': 'en'}, page_content='Short-term memory - Docs by LangChainSkip to main contentWe\\'ve raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsShort-term memoryLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityEnglishcloseOn this pageOverviewUsageIn productionCustomizing agent memoryCommon patternsTrim messagesDelete messagesSummarize messagesAccess memoryToolsRead short-term memory in a toolWrite short-term memory from toolsPromptBefore modelAfter modelCore componentsShort-term memoryCopy pageCopy page\\u200bOverview\\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.\\nShort term memory lets your application remember previous interactions within a single thread or conversation.\\nA thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\\nConversation history is the most common form of short-term memory. Long conversations pose a challenge to today’s LLMs; a full history may not fit inside an LLM’s context window, resulting in an context loss or errors.\\nEven if your model supports the full context length, most LLMs still perform poorly over long contexts. They get “distracted” by stale or off-topic content, all while suffering from slower response times and higher costs.\\nChat models accept context using messages, which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or “forget” stale information.\\n\\u200bUsage\\nTo add short-term memory (thread-level persistence) to an agent, you need to specify a checkpointer when creating an agent.\\nLangChain’s agent manages short-term memory as a part of your agent’s state.By storing these in the graph’s state, the agent can access the full context for a given conversation while maintaining separation between different threads.State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.\\nCopyAsk AIfrom langchain.agents import create_agent\\nfrom langgraph.checkpoint.memory import InMemorySaver  \\n\\n\\nagent = create_agent(\\n    \"gpt-5\",\\n    [get_user_info],\\n    checkpointer=InMemorySaver(),  \\n)\\n\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\\n    {\"configurable\": {\"thread_id\": \"1\"}},  \\n)\\n\\n\\u200bIn production\\nIn production, use a checkpointer backed by a database:\\nCopyAsk AIpip install langgraph-checkpoint-postgres\\n\\nCopyAsk AIfrom langchain.agents import create_agent\\n\\nfrom langgraph.checkpoint.postgres import PostgresSaver  \\n\\n\\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:\\n    checkpointer.setup() # auto create tables in PostgresSql\\n    agent = create_agent(\\n        \"gpt-5\",\\n        [get_user_info],\\n        checkpointer=checkpointer,  \\n    )\\n\\n\\u200bCustomizing agent memory\\nBy default, agents use AgentState to manage short term memory, specifically the conversation history via a messages key.\\nYou can extend AgentState to add additional fields. Custom state schemas are passed to create_agent using the state_schema parameter.\\nCopyAsk AIfrom langchain.agents import create_agent, AgentState\\nfrom langgraph.checkpoint.memory import InMemorySaver\\n\\n\\nclass CustomAgentState(AgentState):  \\n    user_id: str\\n    preferences: dict\\n\\nagent = create_agent(\\n    \"gpt-5\",\\n    [get_user_info],\\n    state_schema=CustomAgentState,  \\n    checkpointer=InMemorySaver(),\\n)\\n\\n# Custom state can be passed in invoke\\nresult = agent.invoke(\\n    {\\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\\n        \"user_id\": \"user_123\",  \\n        \"preferences\": {\"theme\": \"dark\"}  \\n    },\\n    {\"configurable\": {\"thread_id\": \"1\"}})\\n\\n\\u200bCommon patterns\\nWith short-term memory enabled, long conversations can exceed the LLM’s context window. Common solutions are:\\nTrim messagesRemove first or last N messages (before calling LLM)Delete messagesDelete messages from LangGraph state permanentlySummarize messagesSummarize earlier messages in the history and replace them with a summaryCustom strategiesCustom strategies (e.g., message filtering, etc.)\\nThis allows the agent to keep track of the conversation without exceeding the LLM’s context window.\\n\\u200bTrim messages\\nMost LLMs have a maximum supported context window (denominated in tokens).\\nOne way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you’re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the strategy (e.g., keep the last max_tokens) to use for handling the boundary.\\nTo trim message history in an agent, use the @before_model middleware decorator:\\nCopyAsk AIfrom langchain.messages import RemoveMessage\\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langchain.agents import create_agent, AgentState\\nfrom langchain.agents.middleware import before_model\\nfrom langgraph.runtime import Runtime\\nfrom langchain_core.runnables import RunnableConfig\\nfrom typing import Any\\n\\n\\n@before_model\\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\\n    messages = state[\"messages\"]\\n\\n    if len(messages) <= 3:\\n        return None  # No changes needed\\n\\n    first_msg = messages[0]\\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\\n    new_messages = [first_msg] + recent_messages\\n\\n    return {\\n        \"messages\": [\\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\\n            *new_messages\\n        ]\\n    }\\n\\nagent = create_agent(\\n    model,\\n    tools=tools,\\n    middleware=[trim_messages],\\n    checkpointer=InMemorySaver(),\\n)\\n\\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\\n\\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\\nfinal_response = agent.invoke({\"messages\": \"what\\'s my name?\"}, config)\\n\\nfinal_response[\"messages\"][-1].pretty_print()\\n\"\"\"\\n================================== Ai Message ==================================\\n\\nYour name is Bob. You told me that earlier.\\nIf you\\'d like me to call you a nickname or use a different name, just say the word.\\n\"\"\"\\n\\n\\u200bDelete messages\\nYou can delete messages from the graph state to manage the message history.\\nThis is useful when you want to remove specific messages or clear the entire message history.\\nTo delete messages from the graph state, you can use the RemoveMessage.\\nFor RemoveMessage to work, you need to use a state key with add_messages reducer.\\nThe default AgentState provides this.\\nTo remove specific messages:\\nCopyAsk AIfrom langchain.messages import RemoveMessage  \\n\\ndef delete_messages(state):\\n    messages = state[\"messages\"]\\n    if len(messages) > 2:\\n        # remove the earliest two messages\\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \\n\\nTo remove all messages:\\nCopyAsk AIfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\\n\\ndef delete_messages(state):\\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}  \\n\\nWhen deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you’re using. For example:\\nSome providers expect message history to start with a user message\\nMost providers require assistant messages with tool calls to be followed by corresponding tool result messages.\\n\\nCopyAsk AIfrom langchain.messages import RemoveMessage\\nfrom langchain.agents import create_agent, AgentState\\nfrom langchain.agents.middleware import after_model\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.runtime import Runtime\\nfrom langchain_core.runnables import RunnableConfig\\n\\n\\n@after_model\\ndef delete_old_messages(state: AgentState, runtime: Runtime) -> dict | None:\\n    \"\"\"Remove old messages to keep conversation manageable.\"\"\"\\n    messages = state[\"messages\"]\\n    if len(messages) > 2:\\n        # remove the earliest two messages\\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\\n    return None\\n\\n\\nagent = create_agent(\\n    \"gpt-5-nano\",\\n    tools=[],\\n    system_prompt=\"Please be concise and to the point.\",\\n    middleware=[delete_old_messages],\\n    checkpointer=InMemorySaver(),\\n)\\n\\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I\\'m bob\"}]},\\n    config,\\n    stream_mode=\"values\",\\n):\\n    print([(message.type, message.content) for message in event[\"messages\"]])\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what\\'s my name?\"}]},\\n    config,\\n    stream_mode=\"values\",\\n):\\n    print([(message.type, message.content) for message in event[\"messages\"]])\\n\\nCopyAsk AI[(\\'human\\', \"hi! I\\'m bob\")]\\n[(\\'human\\', \"hi! I\\'m bob\"), (\\'ai\\', \\'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.\\')]\\n[(\\'human\\', \"hi! I\\'m bob\"), (\\'ai\\', \\'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.\\'), (\\'human\\', \"what\\'s my name?\")]\\n[(\\'human\\', \"hi! I\\'m bob\"), (\\'ai\\', \\'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.\\'), (\\'human\\', \"what\\'s my name?\"), (\\'ai\\', \\'Your name is Bob. How can I help you today, Bob?\\')]\\n[(\\'human\\', \"what\\'s my name?\"), (\\'ai\\', \\'Your name is Bob. How can I help you today, Bob?\\')]\\n\\n\\u200bSummarize messages\\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue.\\nBecause of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\\n\\nTo summarize message history in an agent, use the built-in SummarizationMiddleware:\\nCopyAsk AIfrom langchain.agents import create_agent\\nfrom langchain.agents.middleware import SummarizationMiddleware\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langchain_core.runnables import RunnableConfig\\n\\n\\ncheckpointer = InMemorySaver()\\n\\nagent = create_agent(\\n    model=\"gpt-4o\",\\n    tools=[],\\n    middleware=[\\n        SummarizationMiddleware(\\n            model=\"gpt-4o-mini\",\\n            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\\n            messages_to_keep=20,  # Keep last 20 messages after summary\\n        )\\n    ],\\n    checkpointer=checkpointer,\\n)\\n\\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\\nfinal_response = agent.invoke({\"messages\": \"what\\'s my name?\"}, config)\\n\\nfinal_response[\"messages\"][-1].pretty_print()\\n\"\"\"\\n================================== Ai Message ==================================\\n\\nYour name is Bob!\\n\"\"\"\\n\\nSee SummarizationMiddleware for more configuration options.\\n\\u200bAccess memory\\nYou can access and modify the short-term memory (state) of an agent in several ways:\\n\\u200bTools\\n\\u200bRead short-term memory in a tool\\nAccess short term memory (state) in a tool using the ToolRuntime parameter.\\nThe tool_runtime parameter is hidden from the tool signature (so the model doesn’t see it), but the tool can access the state through it.\\nCopyAsk AIfrom langchain.agents import create_agent, AgentState\\nfrom langchain.tools import tool, ToolRuntime\\n\\n\\nclass CustomState(AgentState):\\n    user_id: str\\n\\n@tool\\ndef get_user_info(\\n    runtime: ToolRuntime\\n) -> str:\\n    \"\"\"Look up user info.\"\"\"\\n    user_id = runtime.state[\"user_id\"]\\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\\n\\nagent = create_agent(\\n    model=\"gpt-5-nano\",\\n    tools=[get_user_info],\\n    state_schema=CustomState,\\n)\\n\\nresult = agent.invoke({\\n    \"messages\": \"look up user information\",\\n    \"user_id\": \"user_123\"\\n})\\nprint(result[\"messages\"][-1].content)\\n# > User is John Smith.\\n\\n\\u200bWrite short-term memory from tools\\nTo modify the agent’s short-term memory (state) during execution, you can return state updates directly from the tools.\\nThis is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.\\nCopyAsk AIfrom langchain.tools import tool, ToolRuntime\\nfrom langchain_core.runnables import RunnableConfig\\nfrom langchain.messages import ToolMessage\\nfrom langchain.agents import create_agent, AgentState\\nfrom langgraph.types import Command\\nfrom pydantic import BaseModel\\n\\n\\nclass CustomState(AgentState):  \\n    user_name: str\\n\\nclass CustomContext(BaseModel):\\n    user_id: str\\n\\n@tool\\ndef update_user_info(\\n    runtime: ToolRuntime[CustomContext, CustomState],\\n) -> Command:\\n    \"\"\"Look up and update user info.\"\"\"\\n    user_id = runtime.context.user_id \\n    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\\n    return Command(update={  \\n        \"user_name\": name,\\n        # update the message history\\n        \"messages\": [\\n            ToolMessage(\\n                \"Successfully looked up user information\",\\n                tool_call_id=runtime.tool_call_id\\n            )\\n        ]\\n    })\\n\\n@tool\\ndef greet(\\n    runtime: ToolRuntime[CustomContext, CustomState]\\n) -> str:\\n    \"\"\"Use this to greet the user once you found their info.\"\"\"\\n    user_name = runtime.state[\"user_name\"]\\n    return f\"Hello {user_name}!\"\\n\\nagent = create_agent(\\n    model=\"gpt-5-nano\",\\n    tools=[update_user_info, greet],\\n    state_schema=CustomState, \\n    context_schema=CustomContext,  \\n)\\n\\nagent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]},\\n    context=CustomContext(user_id=\"user_123\"),\\n)\\n\\n\\u200bPrompt\\nAccess short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.\\nCopyAsk AIfrom langchain.agents import create_agent\\nfrom typing import TypedDict\\nfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n\\nclass CustomContext(TypedDict):\\n    user_name: str\\n\\n\\ndef get_weather(city: str) -> str:\\n    \"\"\"Get the weather in a city.\"\"\"\\n    return f\"The weather in {city} is always sunny!\"\\n\\n\\n@dynamic_prompt\\ndef dynamic_system_prompt(request: ModelRequest) -> str:\\n    user_name = request.runtime.context[\"user_name\"]\\n    system_prompt = f\"You are a helpful assistant. Address the user as {user_name}.\"\\n    return system_prompt\\n\\n\\nagent = create_agent(\\n    model=\"gpt-5-nano\",\\n    tools=[get_weather],\\n    middleware=[dynamic_system_prompt],\\n    context_schema=CustomContext,\\n)\\n\\nresult = agent.invoke(\\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\\n    context=CustomContext(user_name=\"John Smith\"),\\n)\\nfor msg in result[\"messages\"]:\\n    msg.pretty_print()\\n\\n\\nOutputCopyAsk AI================================ Human Message =================================\\n\\nWhat is the weather in SF?\\n================================== Ai Message ==================================\\nTool Calls:\\n  get_weather (call_WFQlOGn4b2yoJrv7cih342FG)\\n Call ID: call_WFQlOGn4b2yoJrv7cih342FG\\n  Args:\\n    city: San Francisco\\n================================= Tool Message =================================\\nName: get_weather\\n\\nThe weather in San Francisco is always sunny!\\n================================== Ai Message ==================================\\n\\nHi John Smith, the weather in San Francisco is always sunny!\\n\\n\\u200bBefore model\\nAccess short term memory (state) in @before_model middleware to process messages before model calls.\\n\\nCopyAsk AIfrom langchain.messages import RemoveMessage\\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langchain.agents import create_agent, AgentState\\nfrom langchain.agents.middleware import before_model\\nfrom langgraph.runtime import Runtime\\nfrom typing import Any\\n\\n\\n@before_model\\ndef trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\\n    \"\"\"Keep only the last few messages to fit context window.\"\"\"\\n    messages = state[\"messages\"]\\n\\n    if len(messages) <= 3:\\n        return None  # No changes needed\\n\\n    first_msg = messages[0]\\n    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]\\n    new_messages = [first_msg] + recent_messages\\n\\n    return {\\n        \"messages\": [\\n            RemoveMessage(id=REMOVE_ALL_MESSAGES),\\n            *new_messages\\n        ]\\n    }\\n\\nagent = create_agent(\\n    model,\\n    tools=tools,\\n    middleware=[trim_messages]\\n)\\n\\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\\n\\nagent.invoke({\"messages\": \"hi, my name is bob\"}, config)\\nagent.invoke({\"messages\": \"write a short poem about cats\"}, config)\\nagent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\\nfinal_response = agent.invoke({\"messages\": \"what\\'s my name?\"}, config)\\n\\nfinal_response[\"messages\"][-1].pretty_print()\\n\"\"\"\\n================================== Ai Message ==================================\\n\\nYour name is Bob. You told me that earlier.\\nIf you\\'d like me to call you a nickname or use a different name, just say the word.\\n\"\"\"\\n\\n\\u200bAfter model\\nAccess short term memory (state) in @after_model middleware to process messages after model calls.\\n\\nCopyAsk AIfrom langchain.messages import RemoveMessage\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langchain.agents import create_agent, AgentState\\nfrom langchain.agents.middleware import after_model\\nfrom langgraph.runtime import Runtime\\n\\n\\n@after_model\\ndef validate_response(state: AgentState, runtime: Runtime) -> dict | None:\\n    \"\"\"Remove messages containing sensitive words.\"\"\"\\n    STOP_WORDS = [\"password\", \"secret\"]\\n    last_message = state[\"messages\"][-1]\\n    if any(word in last_message.content for word in STOP_WORDS):\\n        return {\"messages\": [RemoveMessage(id=last_message.id)]}\\n    return None\\n\\nagent = create_agent(\\n    model=\"gpt-5-nano\",\\n    tools=[],\\n    middleware=[validate_response],\\n    checkpointer=InMemorySaver(),\\n)\\n\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoToolsPreviousStreamingNextDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://docs.langchain.com/oss/python/langchain/overview\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/rag\",\n",
    "    \"https://docs.langchain.com/oss/python/langchain/short-term-memory\"\n",
    "\n",
    "]\n",
    "\n",
    "docs = WebBaseLoader(urls).load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f51adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the docs\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9fcaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_vector_store = FAISS.from_documents(\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    documents=doc_splits\n",
    "\n",
    ")\n",
    "\n",
    "langchain_retriever = langchain_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef073d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_retriever_tool = create_retriever_tool(\n",
    "    langchain_retriever,\n",
    "    \"retriever_tool_vector_langchain_blog\",\n",
    "    \"Search and learn information about langchain\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69e75719",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [langgraph_retriever_tool, langchain_retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c059a",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7949d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca4b9fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! 👋 How can I help you today?', additional_kwargs={'reasoning_content': 'User says \"Hi\". It\\'s a greeting. We need to respond with a friendly greeting.'}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 72, 'total_tokens': 111, 'completion_time': 0.038507038, 'prompt_time': 0.003350416, 'queue_time': 0.047895651, 'total_time': 0.041857454, 'completion_tokens_details': {'reasoning_tokens': 19}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--fe96e4be-bb1e-43e6-b855-bf06702c4eb8-0', usage_metadata={'input_tokens': 72, 'output_tokens': 39, 'total_tokens': 111})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-20b\")\n",
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cedaca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bind tools with llm\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffbd89b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: AgentState):\n",
    "    \"\"\"Invokes the agent model to generate a response based on the current state. Given the question, it will decide to retrive using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"--- CALL AGENT ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6818ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langsmith import Client\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65224d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data model\n",
    "class grade(BaseModel):\n",
    "    \"\"\" binary score for relavance check \"\"\"\n",
    "    binary_score: str = Field(description=\"Relevence score 'yes' or 'no' \")\n",
    "\n",
    "# Routing function\n",
    "def grade_documents(state: AgentState) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"Determines whether the retrieved documents are relavant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): the current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "    print(\"--- CHECK RELAVANCE ---\")\n",
    "\n",
    "    # setup the prompt\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=\"\"\"\n",
    "    You are a grade assessing relevance of a retrieved document to a user question. \\n\n",
    "    Here is the retrieved document: \\n\\n {context}\\n\\n\n",
    "    Here is the user question: {question}: \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relavant. \\n give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # create chain\n",
    "    chain = prompt | llm_with_tools\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    score = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    if score == 'yes':\n",
    "        print(\"--- DECISION: DOCS RELEVANT ---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"--- DOCS ARE IRRELEVANT ---\")\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93329a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate function\n",
    "def generate(state: AgentState):\n",
    "    \"\"\" \n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current State\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated message\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"--- GENERATE ---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    prompt = Client(api_key=os.getenv('LANGCHAIN_API_KEY')\n",
    "                    ).pull_prompt(\"rlm/rag-prompt\")\n",
    "\n",
    "    # chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64bb07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state: AgentState):\n",
    "    \"\"\"Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: the updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print('--- TRANSFORM QUERY ---')\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n Look at the input and try to reason about the underlying semantic intent/ meaning. \\n\n",
    "            Here is the initial question:\n",
    "            \\n --------\\n\n",
    "            {question}\n",
    "            \\n---------\\n\n",
    "            Formulate an improved question: \n",
    "            \"\"\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # grader\n",
    "    response = llm.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90c7228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt.tool_node import ToolNode, tools_condition\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# define the nodes\n",
    "workflow.add_node(\"agent\", agent)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# add edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", tools_condition)\n",
    "workflow.add_conditional_edges(\"tools\", grade_documents)\n",
    "\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3179671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74a5009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CALL AGENT ---\n",
      "--- CHECK RELAVANCE ---\n",
      "--- DOCS ARE IRRELEVANT ---\n",
      "--- TRANSFORM QUERY ---\n",
      "--- CALL AGENT ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is langgraph?', additional_kwargs={}, response_metadata={}, id='af8149b3-f3e8-4563-be23-4a61edd6cf05'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to answer \"What is langgraph?\" The user likely wants an explanation. Use the tools? There\\'s a function retriever_vector_db_blog. That might fetch info about langgraph. Let\\'s use it.', 'tool_calls': [{'id': 'fc_321ac6b1-784a-423b-88f5-1aba5a735630', 'function': {'arguments': '{\"query\":\"langgraph\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 173, 'total_tokens': 244, 'completion_time': 0.07199174, 'prompt_time': 0.115664566, 'queue_time': 0.051736804, 'total_time': 0.187656306, 'completion_tokens_details': {'reasoning_tokens': 43}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--f7828921-cbae-4735-b710-fd3fb5f7ab4e-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'langgraph'}, 'id': 'fc_321ac6b1-784a-423b-88f5-1aba5a735630', 'type': 'tool_call'}], usage_metadata={'input_tokens': 173, 'output_tokens': 71, 'total_tokens': 244}),\n",
       "  ToolMessage(content=\"LangGraph overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this page InstallCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageCopy pageLangGraph v1.0 is now available!For a complete list of changes and instructions on how to upgrade your code, see the release notes and migration guide.If you encounter any issues or have feedback, please\\n\\nThinking in LangGraph - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedThinking in LangGraphLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this pageStart with the process you want to automateStep 1: Map out your workflow as discrete stepsStep 2: Identify what each step needs to doLLM StepsData StepsAction StepsUser Input StepsStep 3: Design your stateWhat belongs in state?Keep state raw, format prompts on-demandStep 4: Build\\n\\nWorkflows and agents - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationGet startedWorkflows and agentsLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewLangGraph v1.0Release notesMigration guideGet startedInstallQuickstartLocal serverThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureStudioTestDeployAgent Chat UIObservabilityLangGraph APIsGraph APIFunctional APIRuntimeEnglishcloseOn this pageSetupLLMs and augmentationsPrompt chainingParallelizationRoutingOrchestrator-workerCreating workers in LangGraphEvaluator-optimizerAgentsGet startedWorkflows and agentsCopy pageCopy pageThis guide reviews common workflow and agent patterns.\\n\\nTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents.\\nLangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools.\\nWe will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops.\\nLangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more.\\n\\u200b Install\\npipuvCopyAsk AIpip install -U langgraph\", name='retriever_vector_db_blog', id='867e6afc-8d20-4bbc-b8dd-dcae318a21c4', tool_call_id='fc_321ac6b1-784a-423b-88f5-1aba5a735630'),\n",
       "  AIMessage(content='Could you explain what LangGraph is and how it is used to build and orchestrate language‑model workflows?', additional_kwargs={'reasoning_content': 'We need to produce an improved question about \"What is langgraph?\" The instruction: \"Look at the input and try to reason about the underlying semantic intent/ meaning. Formulate an improved question.\" So we need to rephrase the question to be more specific or clarify. The initial question is \"What is langgraph?\" We need to produce an improved question, likely clarifying what context: maybe asking for a definition, usage, features, etc. So improved question: \"Could you explain what LangGraph is and its primary functions in building language model applications?\" Or \"What is LangGraph, and how does it facilitate the construction of language model workflows?\" Something like that. Provide an improved question. The user didn\\'t specify context, but the improved question should be more precise. Let\\'s produce a refined question.'}, response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens': 116, 'total_tokens': 310, 'completion_time': 0.225564296, 'prompt_time': 0.00557059, 'queue_time': 0.04800785, 'total_time': 0.231134886, 'completion_tokens_details': {'reasoning_tokens': 163}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--a2d820b4-0bac-422e-9260-349c1cdd113a-0', usage_metadata={'input_tokens': 116, 'output_tokens': 194, 'total_tokens': 310}),\n",
       "  AIMessage(content='## What is LangGraph?\\n\\nLangGraph is a **low‑level, open‑source framework** for building, managing, and deploying *stateful, long‑running* agents that interact with language models (LLMs) and other tools.  \\nIt sits under the LangChain ecosystem but is **independent**—you can use it with any LLM, prompt‑engineering library, or tooling stack.  \\n\\n> **Key idea** – Treat an agent as a *state‑ful graph* of steps.  \\n> Each node does a small, well‑defined job (e.g., “generate a prompt,” “call an API,” “wait for user input”).  \\n> The graph keeps track of the *state* that flows between nodes, supports branching, looping, and can be persisted so that an agent can pause, resume, or be inspected later.\\n\\n---\\n\\n## Core Concepts\\n\\n| Concept | What it is | Why it matters |\\n|---------|------------|----------------|\\n| **Node** | A callable (Python function, LLM prompt, tool, etc.) that receives the current state and returns a new state. | Encapsulates a single step of logic. |\\n| **Edge** | The rule that decides which node to run next, based on the current state. | Enables branching, loops, and conditional flows. |\\n| **State** | A dictionary that holds all data needed by the graph (inputs, intermediate results, metadata). | Keeps everything deterministic and serializable. |\\n| **Graph** | The directed acyclic graph (DAG) of nodes + edges. | The *blueprint* of the entire workflow. |\\n| **Runtime** | The engine that executes a graph, handles streaming, persistence, and retries. | Turns the blueprint into a live, observable agent. |\\n| **Persisted State** | Serialized state stored in a database or file. | Allows long‑running, resumable agents (e.g., 24‑hour customer support bot). |\\n| **Subgraph** | A reusable graph that can be embedded in another graph. | Promotes composability (e.g., “summarization sub‑workflow”). |\\n| **Time‑Travel** | Ability to rewind to a previous state and explore alternative paths. | Debugging, auditing, or “what‑if” analysis. |\\n\\n---\\n\\n## Typical Workflow\\n\\n1. **Map out the steps**  \\n   *E.g.*:  \\n   1. Get user question → 2. Generate a prompt → 3. Call LLM → 4. Post‑process → 5. Return answer.\\n\\n2. **Define the state**  \\n   ```python\\n   state = {\\n       \"question\": None,\\n       \"prompt\": None,\\n       \"llm_response\": None,\\n       \"final_answer\": None,\\n   }\\n   ```\\n\\n3. **Create nodes**  \\n   ```python\\n   def get_user_input(state):\\n       state[\"question\"] = input(\"Ask me anything: \")\\n       return state\\n\\n   def build_prompt(state):\\n       state[\"prompt\"] = f\"Answer concisely: {state[\\'question\\']}\"\\n       return state\\n\\n   def call_llm(state):\\n       from langchain.chat_models import ChatOpenAI\\n       llm = ChatOpenAI()\\n       state[\"llm_response\"] = llm(state[\"prompt\"])\\n       return state\\n\\n   def post_process(state):\\n       state[\"final_answer\"] = state[\"llm_response\"].strip()\\n       return state\\n   ```\\n\\n4. **Wire edges**  \\n   ```python\\n   from langgraph import Graph, State\\n\\n   g = Graph()\\n   g.add_node(\"input\", get_user_input)\\n   g.add_node(\"prompt\", build_prompt)\\n   g.add_node(\"llm\", call_llm)\\n   g.add_node(\"post\", post_process)\\n\\n   g.add_edge(\"input\", \"prompt\")\\n   g.add_edge(\"prompt\", \"llm\")\\n   g.add_edge(\"llm\", \"post\")\\n   ```\\n\\n5. **Run the graph**  \\n   ```python\\n   result = g.run(state=State(initial_state={}))\\n   print(result[\"final_answer\"])\\n   ```\\n\\n---\\n\\n## Advanced Features\\n\\n| Feature | How it’s used in LangGraph |\\n|---------|----------------------------|\\n| **Durable execution** | Graph state is automatically persisted to a database (e.g., SQLite, Postgres, Redis). If the process crashes, the agent can resume from the last checkpoint. |\\n| **Streaming** | Nodes can yield partial outputs (e.g., streaming LLM tokens). The runtime forwards those to the caller in real time. |\\n| **Human‑in‑the‑Loop** | A node can pause and await external input (e.g., a manager’s approval). The runtime exposes a REST or WebSocket endpoint for the human to respond. |\\n| **Time‑Travel** | After execution, you can inspect the state history and jump back to any step, re‑run from there with different inputs, or compare alternative branches. |\\n| **Subgraphs & Composition** | Reuse common patterns (e.g., “fetch data → clean → analyze”) across multiple agents. |\\n| **Observability** | Built‑in logging, tracing, and integration with LangSmith for real‑time monitoring. |\\n| **Parallelism** | Multiple nodes can run concurrently if they don’t depend on each other. |\\n| **Tool‑Calling** | Wrap external APIs or Python functions as nodes, enabling the agent to perform actions beyond text generation. |\\n\\n---\\n\\n## Integration with LangChain\\n\\n- **Models & Tools**: Use LangChain’s `ChatOpenAI`, `LLMChain`, or `Tool` classes inside LangGraph nodes.  \\n- **Prompt Templates**: Keep prompts in LangChain’s `PromptTemplate` and pass them to nodes.  \\n- **Memory**: Persist chat history or other contextual data in LangChain’s `BaseMemory` and expose it as part of the graph state.\\n\\nExample of a LangChain tool inside a LangGraph node:\\n\\n```python\\nfrom langchain.tools import Tool\\nfrom langgraph import Graph\\n\\nsearch_tool = Tool(\\n    name=\"WebSearch\",\\n    description=\"Search the web for a query\",\\n    func=web_search_function,\\n)\\n\\ndef call_search(state):\\n    query = state[\"question\"]\\n    state[\"search_results\"] = search_tool.run(query)\\n    return state\\n```\\n\\n---\\n\\n## When to Use LangGraph\\n\\n| Scenario | Why LangGraph? |\\n|----------|----------------|\\n| **Long‑running, stateful agents** (e.g., customer support that spans hours) | Durable execution & state persistence. |\\n| **Complex branching logic** (e.g., “if answer < 3 words, ask for clarification”) | Explicit edges and subgraphs. |\\n| **Human‑in‑the‑loop workflows** | Pauses and awaits external input. |\\n| **Observability & debugging** | Time‑travel, logging, and LangSmith integration. |\\n| **Reusable components** | Build subgraphs once and import into many agents. |\\n\\nIf you only need a quick “chat‑bot” or a simple tool‑calling loop, LangChain’s higher‑level agent abstractions may be enough. LangGraph shines when you need fine‑grained control, persistence, or custom orchestration logic.\\n\\n---\\n\\n## Quick Starter\\n\\n```bash\\npip install langgraph langchain openai\\n```\\n\\n```python\\n# quick_demo.py\\nfrom langgraph import Graph, State\\nfrom langchain.chat_models import ChatOpenAI\\n\\ndef get_input(state):\\n    state[\"question\"] = input(\"Ask a question: \")\\n    return state\\n\\ndef build_prompt(state):\\n    state[\"prompt\"] = f\"Answer briefly: {state[\\'question\\']}\"\\n    return state\\n\\ndef call_llm(state):\\n    llm = ChatOpenAI()\\n    state[\"response\"] = llm(state[\"prompt\"])\\n    return state\\n\\ndef finish(state):\\n    print(\"Answer:\", state[\"response\"])\\n    return state\\n\\ng = Graph()\\ng.add_node(\"input\", get_input)\\ng.add_node(\"prompt\", build_prompt)\\ng.add_node(\"llm\", call_llm)\\ng.add_node(\"finish\", finish)\\n\\ng.add_edge(\"input\", \"prompt\")\\ng.add_edge(\"prompt\", \"llm\")\\ng.add_edge(\"llm\", \"finish\")\\n\\ng.run()\\n```\\n\\nRun it:\\n\\n```bash\\n$ python quick_demo.py\\nAsk a question: What is LangGraph?\\nAnswer: LangGraph is a framework for building stateful language‑model agents.\\n```\\n\\n---\\n\\n## Take‑away\\n\\n* **LangGraph** = *Graph‑based orchestration* for LLM agents.  \\n* **Nodes + edges + state** give you the full control over how data flows and how decisions are made.  \\n* Built‑in durability, streaming, and observability make it suitable for production‑grade, long‑running agents.  \\n* It plays nicely with LangChain components, but you’re not forced to use LangChain.  \\n\\nHappy building!', additional_kwargs={'reasoning_content': 'User asks: \"Could you explain what LangGraph is and how it is used to build and orchestrate language‑model workflows?\" They want an explanation. We should provide overview, key concepts, usage patterns, integration with LangChain, features like durable execution, streaming, subgraphs, time travel, memory, etc. Provide practical example. Use concise but thorough.'}, response_metadata={'token_usage': {'completion_tokens': 1928, 'prompt_tokens': 1035, 'total_tokens': 2963, 'completion_time': 2.079615721, 'prompt_time': 0.06950914, 'queue_time': 0.047224722, 'total_time': 2.1491248609999998, 'completion_tokens_details': {'reasoning_tokens': 74}}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--2f21edae-76c7-48ec-a7ee-45a7f052ba30-0', usage_metadata={'input_tokens': 1035, 'output_tokens': 1928, 'total_tokens': 2963})]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\": \"What is langgraph?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb004a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-ai-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
